
```
mkdir NuExtractTiny1.5
cd NuExtractTiny1.5
python312 -m venv venv
```

 NuExtractTiny1.5 ◉
venv\Scripts\activate

(venv)  NuExtractTiny1.5 ◉
pip install openai streamlit==1.40.1 tiktoken

Download quantized model from https://huggingface.co/bartowski/NuExtract-1.5-smol-GGUF/tree/main
NuExtract-1.5-smol-Q5_K_L.gguf



## NuExtract-1.5-smol by NuMind 🔥
NuExtract-1.5-smol is a fine-tuning of Hugging Face's SmolLM2-1.7B, intended for structured information extraction. It uses the same training data as NuExtract-1.5 and supports multiple languages, while being less than half the size (1.7B vs 3.8B).

To use the model, provide an input text and a JSON template describing the information you need to extract.

Note: This model is trained to prioritize pure extraction, so in most cases all text generated by the model is present as is in the original text.

Check out the blog post.

Try the 3.8B model here: Playground

We also provide a tiny (0.5B) version which is based on Qwen2.5-0.5B: NuExtract-tiny-v1.5

⚠️ We recommend using NuExtract with a temperature at or very close to 0. Some inference frameworks, such as Ollama, use a default of 0.7 which is not well suited to pure extraction tasks.

zero shot english
image https://huggingface.co/numind/NuExtract-1.5-smol/resolve/main/english_bench.png

zero shot multilingual
https://huggingface.co/numind/NuExtract-1.5-smol/resolve/main/multilingual_bench.png


Run the server
```
 llama.cpp ◉
> .\llama-server.exe --port 8001 -m .\model\NuExtract-1.5-smol-Q5_K_L.gguf -ngl 25 -c 8196
```

API ENDOPiNT: https://github.com/ggerganov/llama.cpp/tree/master/examples/server
OpenAI API standard > https://github.com/openai/openai-openapi

EOS TOKEN
```
'<|endoftext|>'
```

LOG MODEL
```
llama_load_model_from_file: using device Vulkan0 (Intel(R) UHD Graphics) - 8079 MiB free
llama_model_loader: loaded meta data with 41 key-value pairs and 218 tensors from .\model\NuExtract-1.5-smol-Q5_K_L.gguf (version GGUF V3 (latest))
llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.
llama_model_loader: - kv   0:                       general.architecture str              = llama
llama_model_loader: - kv   1:                               general.type str              = model
llama_model_loader: - kv   2:                               general.name str              = NuExtract 1.5 Smol
llama_model_loader: - kv   3:                       general.organization str              = Numind
llama_model_loader: - kv   4:                         general.size_label str              = 1.7B
llama_model_loader: - kv   5:                            general.license str              = mit
llama_model_loader: - kv   6:                   general.base_model.count u32              = 1
llama_model_loader: - kv   7:                  general.base_model.0.name str              = SmolLM2 1.7B
llama_model_loader: - kv   8:          general.base_model.0.organization str              = HuggingFaceTB
llama_model_loader: - kv   9:              general.base_model.0.repo_url str              = https://huggingface.co/HuggingFaceTB/...
llama_model_loader: - kv  10:                               general.tags arr[str,2]       = ["nlp", "text-generation"]
llama_model_loader: - kv  11:                          general.languages arr[str,1]       = ["multilingual"]
llama_model_loader: - kv  12:                          llama.block_count u32              = 24
llama_model_loader: - kv  13:                       llama.context_length u32              = 8192
llama_model_loader: - kv  14:                     llama.embedding_length u32              = 2048
llama_model_loader: - kv  15:                  llama.feed_forward_length u32              = 8192
llama_model_loader: - kv  16:                 llama.attention.head_count u32              = 32
llama_model_loader: - kv  17:              llama.attention.head_count_kv u32              = 32
llama_model_loader: - kv  18:                       llama.rope.freq_base f32              = 130000.000000
llama_model_loader: - kv  19:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010
llama_model_loader: - kv  20:                 llama.attention.key_length u32              = 64
llama_model_loader: - kv  21:               llama.attention.value_length u32              = 64
llama_model_loader: - kv  22:                          general.file_type u32              = 17
llama_model_loader: - kv  23:                           llama.vocab_size u32              = 49152
llama_model_loader: - kv  24:                 llama.rope.dimension_count u32              = 64
llama_model_loader: - kv  25:                       tokenizer.ggml.model str              = gpt2
llama_model_loader: - kv  26:                         tokenizer.ggml.pre str              = smollm
llama_model_loader: - kv  27:                      tokenizer.ggml.tokens arr[str,49152]   = ["<|endoftext|>", "<|im_start|>", "<|...
llama_model_loader: - kv  28:                  tokenizer.ggml.token_type arr[i32,49152]   = [3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, ...
llama_model_loader: - kv  29:                      tokenizer.ggml.merges arr[str,48900]   = ["─á t", "─á a", "i n", "h e", "─á ─á...
llama_model_loader: - kv  30:                tokenizer.ggml.bos_token_id u32              = 0
llama_model_loader: - kv  31:                tokenizer.ggml.eos_token_id u32              = 0
llama_model_loader: - kv  32:            tokenizer.ggml.unknown_token_id u32              = 0
llama_model_loader: - kv  33:            tokenizer.ggml.padding_token_id u32              = 0
llama_model_loader: - kv  34:            tokenizer.ggml.add_space_prefix bool             = false
llama_model_loader: - kv  35:               tokenizer.ggml.add_bos_token bool             = false
llama_model_loader: - kv  36:               general.quantization_version u32              = 2
llama_model_loader: - kv  37:                      quantize.imatrix.file str              = /models_out/NuExtract-1.5-smol-GGUF/N...
llama_model_loader: - kv  38:                   quantize.imatrix.dataset str              = /training_dir/calibration_datav3.txt
llama_model_loader: - kv  39:             quantize.imatrix.entries_count i32              = 168
llama_model_loader: - kv  40:              quantize.imatrix.chunks_count i32              = 141
llama_model_loader: - type  f32:   49 tensors
llama_model_loader: - type q8_0:    1 tensors
llama_model_loader: - type q5_K:  144 tensors
llama_model_loader: - type q6_K:   24 tensors
llm_load_vocab: special tokens cache size = 17
llm_load_vocab: token to piece cache size = 0.3170 MB
llm_load_print_meta: format           = GGUF V3 (latest)
llm_load_print_meta: arch             = llama
llm_load_print_meta: vocab type       = BPE
llm_load_print_meta: n_vocab          = 49152
llm_load_print_meta: n_merges         = 48900
llm_load_print_meta: vocab_only       = 0
llm_load_print_meta: n_ctx_train      = 8192
llm_load_print_meta: n_embd           = 2048
llm_load_print_meta: n_layer          = 24
llm_load_print_meta: n_head           = 32
llm_load_print_meta: n_head_kv        = 32
llm_load_print_meta: n_rot            = 64
llm_load_print_meta: n_swa            = 0
llm_load_print_meta: n_embd_head_k    = 64
llm_load_print_meta: n_embd_head_v    = 64
llm_load_print_meta: n_gqa            = 1
llm_load_print_meta: n_embd_k_gqa     = 2048
llm_load_print_meta: n_embd_v_gqa     = 2048
llm_load_print_meta: f_norm_eps       = 0.0e+00
llm_load_print_meta: f_norm_rms_eps   = 1.0e-05
llm_load_print_meta: f_clamp_kqv      = 0.0e+00
llm_load_print_meta: f_max_alibi_bias = 0.0e+00
llm_load_print_meta: f_logit_scale    = 0.0e+00
llm_load_print_meta: n_ff             = 8192
llm_load_print_meta: n_expert         = 0
llm_load_print_meta: n_expert_used    = 0
llm_load_print_meta: causal attn      = 1
llm_load_print_meta: pooling type     = 0
llm_load_print_meta: rope type        = 0
llm_load_print_meta: rope scaling     = linear
llm_load_print_meta: freq_base_train  = 130000.0
llm_load_print_meta: freq_scale_train = 1
llm_load_print_meta: n_ctx_orig_yarn  = 8192
llm_load_print_meta: rope_finetuned   = unknown
llm_load_print_meta: ssm_d_conv       = 0
llm_load_print_meta: ssm_d_inner      = 0
llm_load_print_meta: ssm_d_state      = 0
llm_load_print_meta: ssm_dt_rank      = 0
llm_load_print_meta: ssm_dt_b_c_rms   = 0
llm_load_print_meta: model type       = ?B
llm_load_print_meta: model ftype      = Q5_K - Medium
llm_load_print_meta: model params     = 1.71 B
llm_load_print_meta: model size       = 1.16 GiB (5.83 BPW)
llm_load_print_meta: general.name     = NuExtract 1.5 Smol
llm_load_print_meta: BOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOS token        = 0 '<|endoftext|>'
llm_load_print_meta: EOT token        = 0 '<|endoftext|>'
llm_load_print_meta: UNK token        = 0 '<|endoftext|>'
llm_load_print_meta: PAD token        = 0 '<|endoftext|>'
llm_load_print_meta: LF token         = 143 '├ä'
llm_load_print_meta: EOG token        = 0 '<|endoftext|>'
llm_load_print_meta: EOG token        = 2 '<|im_end|>'
llm_load_print_meta: max token length = 162
ggml_vulkan: Compiling shaders..............................Done!
llm_load_tensors: offloading 24 repeating layers to GPU
llm_load_tensors: offloading output layer to GPU
llm_load_tensors: offloaded 25/25 layers to GPU
llm_load_tensors:      Vulkan0 model buffer size =  1190.26 MiB
llm_load_tensors:   CPU_Mapped model buffer size =   102.00 MiB
```